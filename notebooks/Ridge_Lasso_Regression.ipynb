{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9845993",
   "metadata": {},
   "source": [
    "#### This notebook will go through the implementation of Ridge and Lasso regressions from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611228c0",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7ac81",
   "metadata": {},
   "source": [
    "\n",
    "### 1. What is Regularization?\n",
    "\n",
    "Regularization is a technique used in machine learning to prevent overfitting by discouraging overly complex models. It introduces a penalty term to the loss function that the algorithm tries to minimize, which constrains the magnitude of the model parameters (coefficients). By doing so, regularization encourages the model to learn simpler patterns that generalize better to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. L1 and L2 Regularization\n",
    "\n",
    "- **L1 Regularization (Lasso Regression):**\n",
    "  - Adds the **absolute values** of the coefficients as penalty to the loss function.\n",
    "  - Formula: `Loss + α * Σ|wᵢ|`\n",
    "  - Encourages **sparse solutions**—some coefficients may become exactly zero, effectively performing feature selection.\n",
    "\n",
    "- **L2 Regularization (Ridge Regression):**\n",
    "  - Adds the **squared values** of the coefficients as penalty.\n",
    "  - Formula: `Loss + α * Σ(wᵢ²)`\n",
    "  - Tends to **shrink coefficients** evenly but rarely forces them to zero.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why is Regularization Important?\n",
    "\n",
    "- **Reduces Overfitting**: Helps prevent the model from fitting noise in the training data.\n",
    "- **Improves Generalization**: Encourages simpler models that work better on new, unseen data.\n",
    "- **Feature Selection (L1)**: Lasso can automatically eliminate irrelevant features by assigning them zero weight.\n",
    "- **Model Stability**: Regularization makes the model less sensitive to small changes in the input data.\n",
    "\n",
    "---\n",
    "\n",
    "🔑 In summary, regularization is a critical tool in a machine learning practitioner's toolbox to build robust, generalizable models, especially when working with high-dimensional or noisy datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c743301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d1c6f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Comparison Results:\n",
      "\n",
      "Linear Regression (Closed-form)     | MSE: 0.5559 | R²: 0.5758\n",
      "SGDRegressor (Linear GD)            | MSE: 0.5506 | R²: 0.5798\n",
      "Ridge Regression                    | MSE: 0.5559 | R²: 0.5758\n",
      "Lasso Regression                    | MSE: 0.6796 | R²: 0.4814\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load and split the dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Linear Regression (Closed-form)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_preds = lr.predict(X_test_scaled)\n",
    "\n",
    "# Linear Regression with SGDRegressor\n",
    "sgd = SGDRegressor(max_iter=1000, learning_rate='invscaling', eta0=0.01, penalty=None, random_state=42)\n",
    "sgd.fit(X_train_scaled, y_train)\n",
    "sgd_preds = sgd.predict(X_test_scaled)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "ridge_preds = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "lasso_preds = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name:<35} | MSE: {mse:.4f} | R²: {r2:.4f}\")\n",
    "\n",
    "# Compare models\n",
    "print(\"📊 Model Comparison Results:\\n\")\n",
    "evaluate_model(\"Linear Regression (Closed-form)\", y_test, lr_preds)\n",
    "evaluate_model(\"SGDRegressor (Linear GD)\", y_test, sgd_preds)\n",
    "evaluate_model(\"Ridge Regression\", y_test, ridge_preds)\n",
    "evaluate_model(\"Lasso Regression\", y_test, lasso_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a5ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor (Linear GD):\n",
      "0.5506075204179468\n",
      "0.5798200946722103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "        \n",
    "sgd = SGDRegressor(penalty=None, max_iter=1000, learning_rate='invscaling', eta0=0.01, random_state=42)\n",
    "sgd.fit(X_train_scaled, y_train)\n",
    "y_pred_sgd = sgd.predict(X_test_scaled)\n",
    "\n",
    "mse_sgd = mean_squared_error(y_test, y_pred_sgd)\n",
    "r2_sgd = r2_score(y_test, y_pred_sgd)\n",
    "\n",
    "print(\"SGDRegressor (Linear GD):\")\n",
    "\n",
    "print(mse_sgd)\n",
    "print(r2_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae99c99",
   "metadata": {},
   "source": [
    "# Ridge from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debd067",
   "metadata": {},
   "source": [
    "## 📘 Ridge Regression with Gradient Descent\n",
    "\n",
    "### What is Ridge Regression?\n",
    "\n",
    "Ridge Regression is a type of **linear regression** that includes **L2 regularization**. It modifies the standard linear regression loss function by adding a penalty proportional to the **square of the magnitude of the coefficients**.\n",
    "\n",
    "The goal is to **prevent overfitting** by discouraging the model from learning excessively large weights.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Ridge Loss Function with L2 Penalty\n",
    "\n",
    "The objective function Ridge minimizes is:\n",
    "\n",
    "\\[\n",
    "J(\\mathbf{w}) = \\frac{1}{2m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^n w_j^2\n",
    "\\]\n",
    "\n",
    "- \\( m \\): number of training samples\n",
    "- \\( y_i \\): true value\n",
    "- \\( \\hat{y}_i \\): predicted value\n",
    "- \\( w_j \\): model weight (coefficient)\n",
    "- \\( \\alpha \\): regularization strength\n",
    "- The first term is the **mean squared error**\n",
    "- The second term is the **L2 penalty** (sum of squared weights)\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Gradient Descent Update Rule\n",
    "\n",
    "Since the loss function is differentiable, we can apply standard gradient descent. The gradient of the Ridge loss with respect to the weights is:\n",
    "\n",
    "\\[\n",
    "\\nabla J(\\mathbf{w}) = \\frac{1}{m} X^\\top (X\\mathbf{w} - y) + 2\\alpha \\mathbf{w}\n",
    "\\]\n",
    "\n",
    "**Weight Update Rule:**\n",
    "\n",
    "\\[\n",
    "\\mathbf{w} = \\mathbf{w} - \\eta \\cdot \\nabla J(\\mathbf{w})\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\eta \\): learning rate\n",
    "- The regularization term \\( 2\\alpha \\mathbf{w} \\) **shrinks** the weights during training\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Key Properties of Ridge Regression\n",
    "\n",
    "- Encourages **small but non-zero** weights (unlike Lasso, which can set them to zero)\n",
    "- Useful when you have **multicollinearity** or **high-dimensional data**\n",
    "- Bias term is typically **excluded** from regularization\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Why Use Ridge Regression?\n",
    "\n",
    "- Reduces model complexity and variance\n",
    "- Helps prevent overfitting\n",
    "- Does **not perform feature selection** (all features are retained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e34ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 Ridge Regression (Gradient Descent):\n",
      "MSE: 0.5559\n",
      "R² Score: 0.5758\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load and prepare data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Add bias term\n",
    "X_train_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_test_bias = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "\n",
    "# Ridge Regression using Gradient Descent\n",
    "class RidgeRegressionGD:\n",
    "    def __init__(self, alpha=1.0, learning_rate=0.01, n_iterations=1000):\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            predictions = X @ self.weights\n",
    "            errors = predictions - y\n",
    "\n",
    "            # Gradient: add 2*alpha*w for L2 regularization (excluding bias)\n",
    "            gradient = (1/m) * (X.T @ errors) + (2 * self.alpha / m) * np.r_[0, self.weights[1:]]\n",
    "            self.weights -= self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.weights\n",
    "\n",
    "# Train and evaluate\n",
    "ridge_gd = RidgeRegressionGD(alpha=1.0, learning_rate=0.1, n_iterations=1000)\n",
    "ridge_gd.fit(X_train_bias, y_train)\n",
    "preds = ridge_gd.predict(X_test_bias)\n",
    "\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "print(\"📉 Ridge Regression (Gradient Descent):\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1837cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠 Lasso Regression (From Scratch):\n",
      "MSE: 0.5559\n",
      "R² Score: 0.5758\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load and prepare data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Add intercept manually\n",
    "X_train_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_test_bias = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "\n",
    "class LassoRegressionScratch:\n",
    "    def __init__(self, alpha=0.1, max_iter=1000, tol=1e-4):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def soft_thresholding(self, rho, alpha):\n",
    "        if rho < -alpha:\n",
    "            return rho + alpha\n",
    "        elif rho > alpha:\n",
    "            return rho - alpha\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.coef_ = np.zeros(n)\n",
    "        for iteration in range(self.max_iter):\n",
    "            coef_old = self.coef_.copy()\n",
    "            for j in range(n):\n",
    "                X_j = X[:, j]\n",
    "                residual = y - X @ self.coef_ + self.coef_[j] * X_j\n",
    "                rho = np.dot(X_j, residual)\n",
    "\n",
    "                if j == 0:  # Intercept (bias) term - no regularization\n",
    "                    self.coef_[j] = rho / np.dot(X_j, X_j)\n",
    "                else:\n",
    "                    self.coef_[j] = self.soft_thresholding(rho, self.alpha) / np.dot(X_j, X_j)\n",
    "\n",
    "            # Check convergence\n",
    "            if np.sum(np.abs(self.coef_ - coef_old)) < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.coef_\n",
    "\n",
    "# Train Lasso\n",
    "lasso_scratch = LassoRegressionScratch(alpha=0.1)\n",
    "lasso_scratch.fit(X_train_bias, y_train)\n",
    "y_pred = lasso_scratch.predict(X_test_bias)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"🛠 Lasso Regression (From Scratch):\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b5357",
   "metadata": {},
   "source": [
    "## 📘 Lasso Regression with Gradient Descent\n",
    "\n",
    "### Why is Lasso Harder Than Ridge?\n",
    "\n",
    "Lasso regression applies **L1 regularization**, adding a penalty equal to the **absolute values of the coefficients**. Unlike Ridge (which uses squared coefficients), the absolute value function is **not differentiable at 0**, making traditional gradient descent inapplicable directly.\n",
    "\n",
    "To handle this, we use a technique called **Subgradient Descent**, which extends gradient descent to non-differentiable functions.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Lasso Loss Function with L1 Penalty\n",
    "\n",
    "The objective function to minimize is:\n",
    "\n",
    "\\[\n",
    "J(\\mathbf{w}) = \\frac{1}{2m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^n |w_j|\n",
    "\\]\n",
    "\n",
    "- \\( \\hat{y}_i \\) is the model prediction for sample \\( i \\)\n",
    "- \\( \\alpha \\) is the regularization strength\n",
    "- The first term is the mean squared error\n",
    "- The second term is the L1 penalty (sum of absolute values of weights)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Subgradient of the L1 Term\n",
    "\n",
    "Since the derivative of \\( |w_j| \\) is undefined at \\( w_j = 0 \\), we use its **subgradient**:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dw_j} |w_j| =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } w_j > 0 \\\\\n",
    "-1, & \\text{if } w_j < 0 \\\\\n",
    "0, & \\text{if } w_j = 0 \\\\\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "This allows us to apply a modified form of gradient descent, called **subgradient descent**, for optimizing Lasso.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Summary\n",
    "\n",
    "- Lasso Regression adds L1 penalty which **encourages sparsity** in weights.\n",
    "- The **absolute value** function requires using **subgradients** instead of standard gradients.\n",
    "- Lasso is powerful when you want both **regularization and feature selection** in a linear model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71bf49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 Lasso Regression (Gradient Descent):\n",
      "MSE: 0.6795\n",
      "R² Score: 0.4815\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load and prepare data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Add intercept term\n",
    "X_train_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_test_bias = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "\n",
    "# Lasso Regression using Gradient Descent\n",
    "class LassoRegressionGD:\n",
    "    def __init__(self, alpha=0.1, learning_rate=0.01, n_iterations=1000):\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "    def subgradient(self, w):\n",
    "        return np.where(w > 0, 1, np.where(w < 0, -1, 0))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            predictions = X @ self.weights\n",
    "            errors = predictions - y\n",
    "\n",
    "            grad = (1/m) * (X.T @ errors) + self.alpha * self.subgradient(self.weights)\n",
    "            grad[0] -= self.alpha * self.subgradient(self.weights[0])  # exclude bias from regularization\n",
    "            self.weights -= self.learning_rate * grad\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.weights\n",
    "\n",
    "# Train and evaluate\n",
    "lasso_gd = LassoRegressionGD(alpha=0.1, learning_rate=0.01, n_iterations=1000)\n",
    "lasso_gd.fit(X_train_bias, y_train)\n",
    "preds = lasso_gd.predict(X_test_bias)\n",
    "\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "print(\"📉 Lasso Regression (Gradient Descent):\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e56259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
