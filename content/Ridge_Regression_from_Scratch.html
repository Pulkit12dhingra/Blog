<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="Explore how to implement Ridge Regression using gradient descent from scratch. Learn why regularization matters and compare performance against OLS and SGD models." />
  <title>Ridge Regression from Scratch: With Gradient Descent and L2 Regularization | Byte-Sized-Brilliance-AI</title>
  <link rel="icon" type="image/png" href="../img/icon.png" />

  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="preconnect" href="https://www.googletagmanager.com" crossorigin>

  <!-- Open Graph -->
  <meta property="og:title" content="Ridge Regression from Scratch: With Gradient Descent and L2 Regularization" />
  <meta property="og:description" content="Explore how to implement Ridge Regression using gradient descent from scratch. Learn why regularization matters and compare performance against OLS and SGD models." />
  <meta property="og:image" content="../img/social-preview.jpg" />
  <meta property="og:type" content="article" />

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Ridge Regression from Scratch: With Gradient Descent and L2 Regularization" />
  <meta name="twitter:description" content="Explore how to implement Ridge Regression using gradient descent from scratch. Learn why regularization matters and compare performance against OLS and SGD models." />
  <meta name="twitter:image" content="../img/social-preview.jpg" />

  <!-- Critical CSS loaded first -->
  <link rel="stylesheet" href="../data/style.css?v=1.0.0" />

  <!-- Non-critical CSS loaded asynchronously -->
  <link
    href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
    rel="stylesheet"
    media="print"
    onload="this.media='all'"
  />
  <noscript>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" />
  </noscript>

  <link
    href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css"
    rel="stylesheet"
    media="print"
    onload="this.media='all'"
  />
  <noscript>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet" />
  </noscript>

  <link
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"
    rel="stylesheet"
    media="print"
    onload="this.media='all'"
  />
  <noscript>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" rel="stylesheet" />
  </noscript>

  <!-- Deferred Google Analytics -->
  <script>
    window.addEventListener('load', function() {
      setTimeout(function() {
        var script = document.createElement('script');
        script.src = 'https://www.googletagmanager.com/gtag/js?id=G-8JMS98M7C7';
        script.async = true;
        script.onload = function() {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-8JMS98M7C7');
        };
        document.head.appendChild(script);
      }, 1500);
    });
  </script>
</head>
      <body>
        <div class="container mt-4">
          <div class="d-flex flex-column flex-md-row justify-content-between align-items-start align-items-md-center mb-4">
            <h2 class="fw-bold mb-3">Ridge Regression from Scratch</h2>
    <a href="../index.html" class="btn btn-primary">
              ‚Üê Back to Home
            </a>
          </div>
          <hr />
        
          <h4 class="mt-4">What is Regularization?</h4>
        <p>
            When building predictive models, it‚Äôs tempting to chase perfection on the training data. But this can lead to <strong>overfitting</strong> ‚Äî where your model memorizes noise instead of learning patterns.
            That‚Äôs where <strong>regularization</strong> comes in.
        </p>
        <p>
            Regularization adds a penalty to the loss function to discourage large, unstable weights ‚Äî nudging the model toward simpler, more generalizable solutions.
        </p>
        <p><strong>Benefits of Regularization:</strong></p>
        <ul>
            <li>Prevents overfitting</li>
            <li>Improves stability and generalization</li>
            <li>Especially helpful in high-dimensional or multicollinear data</li>
        </ul>

        
          <h4 class="mt-4">üìê Ridge Regression (L2)</h4>
          <p>
            Ridge regression adds a penalty to the squared magnitude of weights. This prevents the model from relying too heavily on any single feature:
          </p>
          <pre class="bg-light p-3 rounded"><code>Loss = MSE + Œ± √ó Œ£(w·µ¢¬≤)</code></pre>
          <ul>
            <li><strong>MSE</strong>: Mean Squared Error ‚Äî the average squared difference between predicted and actual target values. This is your primary loss that the model tries to minimize.</li>
            <li><strong>Œ± (alpha)</strong>: Regularization strength ‚Äî a hyperparameter that controls how much penalty you apply to the weights. A higher Œ± means more regularization (more shrinkage).</li>
            <li><strong>w·µ¢</strong>: Each individual weight/parameter (excluding the bias term). These are the model's coefficients.</li>
            <li><strong>Œ£(w·µ¢¬≤)</strong>: The sum of the squares of all weights. This is the L2 penalty ‚Äî it encourages the model to keep weights small and prevents overfitting.</li>
          </ul>
          
          <p>
            So, Ridge regression doesn‚Äôt just minimize prediction error ‚Äî it also penalizes large weights. The goal is a balance between fitting the data well (<code>MSE</code>) and keeping the model simple (<code>Œ± √ó Œ£(w·µ¢¬≤)</code>).
          </p>
        
          <h4 class="mt-4">üè† Load & Prepare the Data</h4>
          <pre class="bg-light p-3 rounded"><code>
        from sklearn.datasets import fetch_california_housing
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        
        housing = fetch_california_housing()
        X, y = housing.data, housing.target
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42)
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Add bias term
        import numpy as np
        X_train_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]
        X_test_bias = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]</code></pre>
        
          <h4 class="mt-4"> Ridge Regression (Gradient Descent Implementation)</h4>
          <pre class="bg-light p-3 rounded"><code>
        class RidgeRegressionGD:
            def __init__(self, alpha=1.0, learning_rate=0.01, n_iterations=1000):
                self.alpha = alpha
                self.learning_rate = learning_rate
                self.n_iterations = n_iterations
        
            def fit(self, X, y):
                m, n = X.shape
                self.weights = np.zeros(n)
                for _ in range(self.n_iterations):
                    predictions = X @ self.weights
                    errors = predictions - y
                    gradient = (1/m) * (X.T @ errors) + (2 * self.alpha / m) * np.r_[0, self.weights[1:]]
                    self.weights -= self.learning_rate * gradient
        
            def predict(self, X):
                return X @ self.weights</code></pre>
        
          <h4 class="mt-4"> Train and Evaluate the Model</h4>
          <pre class="bg-light p-3 rounded"><code>
        from sklearn.metrics import mean_squared_error, r2_score
        
        ridge_gd = RidgeRegressionGD(alpha=1.0, learning_rate=0.1, n_iterations=1000)
        ridge_gd.fit(X_train_bias, y_train)
        preds = ridge_gd.predict(X_test_bias)
        
        mse = mean_squared_error(y_test, preds)
        r2 = r2_score(y_test, preds)
        
        print(" Ridge Regression (Gradient Descent):")
        print(f"MSE: {mse:.4f}")
        print(f"R¬≤ Score: {r2:.4f}")</code></pre>
        <div class="mt-2 mb-4 text-center">
            <img src="../img/Blog_5_1.png" alt="Ridge Gradient Descent Results" class="img-fluid rounded shadow"/>
          </div>
          <h4 class="mt-4"> Compare with OLS and SGD Regressor</h4>
        
          <h5>OLS (LinearRegression)</h5>
          <pre class="bg-light p-3 rounded"><code>
        from sklearn.linear_model import LinearRegression
        
        lr = LinearRegression()
        lr.fit(X_train_scaled, y_train)
        y_pred_lr = lr.predict(X_test_scaled)
        
        mse_lr = mean_squared_error(y_test, y_pred_lr)
        r2_lr = r2_score(y_test, y_pred_lr)</code></pre>
        <div class="mt-2 mb-4 text-center">
            <img src="../img/Blog_5_2.png" alt="OLS Linear Regression Results" class="img-fluid rounded shadow"/>
          </div>
          <h5>SGD Regressor (No Regularization)</h5>
          <pre class="bg-light p-3 rounded"><code>
        from sklearn.linear_model import SGDRegressor
        
        sgd = SGDRegressor(penalty=None, max_iter=1000, learning_rate='invscaling', eta0=0.01, random_state=42)
        sgd.fit(X_train_scaled, y_train)
        y_pred_sgd = sgd.predict(X_test_scaled)
        
        mse_sgd = mean_squared_error(y_test, y_pred_sgd)
        r2_sgd = r2_score(y_test, y_pred_sgd)</code></pre>
        <div class="mt-2 mb-4 text-center">
            <img src="../img/Blog_5_3.png" alt="SGD Regressor Results" class="img-fluid rounded shadow"/>
          </div>

          <h4 class="mt-4"> Results Comparison</h4>
          <ul>
            <li><strong>OLS</strong>: MSE ‚âà <code>0.5559</code>, R¬≤ ‚âà <code>0.5758</code></li>
            <li><strong>Ridge GD</strong>: MSE ‚âà <code>0.5559</code>, R¬≤ ‚âà <code>0.5758</code></li>
            <li><strong>SGD</strong>: MSE ‚âà <code>0.5506</code>, R¬≤ ‚âà <code>0.5798</code></li>
          </ul>
        
          <p> Ridge with gradient descent provides flexibility (custom learning schedules, mini-batching, etc.) while keeping regularization in play.</p>
        
          <h4 class="mt-4"> Reference</h4>
          <p><a href="https://github.com/Pulkit12dhingra/Blog/blob/main/notebooks/Ridge_Lasso_Regression.ipynb" target="_blank">
    </a></p>
</div>

<!-- Footer -->
<footer class="bg-light py-4 mt-5 border-top">
  <div class="container text-center">
    <p class="text-muted mb-1">
      <i class="bi bi-person-circle me-2"></i>
      <strong>Written by:</strong> <a href="https://www.linkedin.com/in/pulkit-dhingra-4b7312193/" target="_blank" rel="noopener noreferrer" class="text-decoration-none">Pulkit Dhingra</a>
    </p>
    <p class="text-muted small mb-0">
      ¬© 2025 Byte-Sized-Brilliance-AI. All rights reserved.
    </p>
  </div>
</footer>

</body>
</html>