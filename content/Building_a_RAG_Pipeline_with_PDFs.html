<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Byte-Sized-Brilliance-AI</title>
  <link rel="icon" type="image/png" href="../img/icon.png" />

  <!-- Meta for sharing -->
  <meta property="og:title" content="Pulkit's Blog" />
  <meta property="og:description" content="Build a Retrieval-Augmented Generation (RAG) pipeline using PDF documents, Chroma DB, and Gemma 3." />
  <meta property="og:image" content="../img/social-preview.jpg" />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Pulkit's Blog" />
  <meta name="twitter:description" content="Build a Retrieval-Augmented Generation (RAG) pipeline using PDF documents, Chroma DB, and Gemma 3." />
  <meta name="twitter:image" content="../img/social-preview.jpg" />

  <!-- CSS and Bootstrap -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" />
  <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet" />
  <link rel="stylesheet" href="data/style.css?v=1.0.0" />
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" rel="stylesheet"/>
</head>
<body>
<div class="container mt-4">
  <div class="d-flex flex-column flex-md-row justify-content-between align-items-start align-items-md-center mb-4">
    <h2 class="fw-bold mb-3">Building a RAG Pipeline with PDFs, Chroma DB, and Gemma 3 🧠📄</h2>
    <a href="../index.html" class="btn btn-primary">← Back to Home</a>
  </div>

  <p class="text-muted">Published: <span id="publish-date">May 9, 2025</span></p>
  <hr />
  <h4 class="mt-4">🧩 Why LangChain?</h4>
    <p>
    LangChain is a powerful framework designed to connect large language models (LLMs) to external data sources, tools, and APIs. 
    If you're just getting started, I recently covered the fundamentals in this post: 
    <a href="https://pulkit12dhingra.github.io/Blog/content/Blog_9.html" target="_blank">Intro to LangChain 🔗</a>.
    It simplifies workflows like Retrieval-Augmented Generation (RAG), allowing you to load documents, embed them, and query intelligently — all in just a few lines of code.
    </p>

  <p>
    Retrieval-Augmented Generation (RAG) is revolutionizing how large language models access and use external data. By combining retrieval mechanisms with powerful LLMs, RAG allows systems to answer questions with information beyond their training data — dynamically pulling facts from your own knowledge base.
  </p>
  <div class="text-center mb-4">
    <img src="../img/Blog_10_2.png" alt="LangChain Logo" class="img-fluid rounded shadow" style="max-width: 400px; width: 100%; height: auto;" />
  </div>
  <p>
    In this tutorial, we’ll build a RAG pipeline using <strong>PDF documents</strong>, store chunked embeddings in <strong>Chroma DB</strong>, and query them using <strong>Ollama with the Gemma 3 model</strong>. Let’s go step-by-step! 🔍
  </p>
  <div class="text-center mb-4">
    <img src="../img/Blog_10_3.png" alt="LangChain Logo" class="img-fluid rounded shadow" style="max-width: 400px; width: 100%; height: auto;" />
  </div>
  <h5 class="mt-4">📄 PDF Source</h5>
<p>
  For this tutorial, the document used for retrieval is my own resume. You can view it here:  
  <a href="https://pulkit12dhingra.github.io/portfolio/resume/Pulkit_Resume.pdf" target="_blank">
    Pulkit_Resume.pdf
  </a>.  
  This makes the RAG pipeline a powerful tool for personal document Q&A and career applications.
</p>


  <h4 class="mt-4">📥 Step 1: Load Your PDFs</h4>
  <p>The first step in any RAG pipeline is loading the data you want your model to "know." In our case, this data comes in the form of PDFs. LangChain provides convenient tools like PyPDFDirectoryLoader that scan a directory and automatically extract text from every PDF file it contains. This abstracts away the complexity of parsing individual files and gives you a list of clean, structured documents — perfect for downstream processing. Whether you're indexing research papers, manuals, or even your resume, this loader simplifies it all into a few lines of code.</p>
  <pre class="bg-light p-3 rounded"><code>
from langchain_community.document_loaders import PyPDFDirectoryLoader

loader = PyPDFDirectoryLoader("./data")
documents = loader.load()
  </code></pre>
  <p>
    We use <code>PyPDFDirectoryLoader</code> to load all PDFs in the <code>./data</code> directory. Each PDF gets parsed into a text document.
  </p>

  <h4 class="mt-4">✂️ Step 2: Split Text into Chunks</h4>
  <p>
    Language models have input size limitations, so feeding them entire documents isn't feasible. That’s where chunking comes in. 
    Using LangChain’s <code>RecursiveCharacterTextSplitter</code>, we break down long documents into manageable segments — or “chunks” — 
    each with a fixed size (500 characters here) and a small overlap (50 characters) to preserve context across boundaries. 
    This step ensures that when the model retrieves text later, it does so with enough context to generate accurate and coherent responses. 
    Chunking is the unsung hero that powers effective document retrieval in any RAG pipeline.
  </p>  
  <pre class="bg-light p-3 rounded"><code>
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(documents)
print(len(chunks))
  </code></pre>
  <h4 class="mt-4">🧠 Step 3: Define Embeddings</h4>
  <p>
    Instead of using an external hosted model, we leverage a local embedding model from HuggingFace: 
    <code>sentence-transformers/all-MiniLM-L6-v2</code>. This model is lightweight, fast, and delivers surprisingly strong performance 
    for semantic search and similarity tasks — making it ideal for use in RAG pipelines.
  </p>
  <pre class="bg-light p-3 rounded"><code>
from langchain.embeddings import HuggingFaceEmbeddings

# Define the Embeddings
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
  </code></pre>
  <div class="mb-4">
    <img src="../img/Blog_10_1.png" alt="embeddings" class="img-fluid rounded shadow" style="max-width: 400px; width: 100%; height: auto;" />
</div>


  <h4 class="mt-4">💾 Step 4: Store Chunks in Chroma DB</h4>
  <p>
    Once we’ve embedded our text chunks, the next crucial step is storing them in a vector database — and that’s where <strong>Chroma DB</strong> comes in. 
    A vectorstore allows us to perform efficient similarity searches based on semantic meaning rather than keyword matches. 
    By persisting the embeddings in Chroma, we enable our RAG pipeline to retrieve the most relevant pieces of information dynamically at query time. 
    Without a vectorstore, the model would have no memory of the document structure or content, making retrieval impossible. 
    In short, the vectorstore is the backbone of the “retrieval” part in Retrieval-Augmented Generation.
  </p>
  
  <pre class="bg-light p-3 rounded"><code>
from langchain.vectorstores import Chroma
# Store chunks in Chroma DB
vectorstore = Chroma.from_documents(chunks, embedding=embedding_model, persist_directory="./chroma_db")
retriever = vectorstore.as_retriever()
  </code></pre>

  <h4 class="mt-4">🔧 Step 5: Initialize Ollama with Gemma 3</h4>
  <p>
    Now we initialize our LLM. <strong>Gemma 3B</strong> is a lightweight and capable model that will be used for the generation step in RAG.
  </p>
  <pre class="bg-light p-3 rounded"><code>
from langchain.llms import Ollama

# Initialize Ollama with the gemma3 model
llm = Ollama(model="gemma3")
  </code></pre>


  <h4 class="mt-4">🔗 Step 6: Create the RAG Pipeline</h4>
  <p>
    This step combines the retriever and the LLM into a unified RAG chain. When queried, it retrieves the most relevant chunks and generates an answer.
  </p>
  <pre class="bg-light p-3 rounded"><code>
from langchain.chains import RetrievalQA

# Create RAG pipeline

rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)
  </code></pre>


  <h4 class="mt-4">❓ Querying the RAG Pipeline</h4>
  <pre class="bg-light p-3 rounded"><code>
# Now we can query it
query = "Has pulkit worked on AWS?" 
response = rag_chain(query)

print("Answer:", response['result'])
  </code></pre>
<h3>Output</h3>
<pre class="bg-light p-3 rounded"><code>Answer: Yes, based on the provided text, 
Pulkit has worked on AWS. Specifically, the Nagarro DevOps Engineer role involved 
“successfully integrated pipelines with Azure Cloud, AWS Cloud, and Google Cloud services.”</code></pre>

  <p>
    Boom! 💥 Now you can ask questions and receive answers grounded in your PDF documents.
  </p>

  <h4 class="mt-4">✅ Final Thoughts</h4>
  <p>
    We’ve just built a full RAG pipeline — from loading PDFs to querying with Gemma 3. This approach gives your LLM access to custom knowledge, perfect for chatbots, document search, internal tools, and more.
  </p>

  <p class="fw-bold">
    Next up: RAG with streaming, multi-turn memory, and UI integration. Stay tuned! 🧠💬
  </p>

</div>
</body>
</html>
