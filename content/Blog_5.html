<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Byte-Sized-Brilliance-AI</title>
        <link rel="icon" type="image/png" href="img/icon.png" />
      
        <!-- Open Graph -->
        <meta property="og:title" content="Pulkit's Blog" />
        <meta property="og:description" content="Explore tutorials, projects, and insights into AI, coding, and more." />
        <meta property="og:image" content="img/social-preview.jpg" />
        <meta property="og:type" content="website" />
      
        <!-- Twitter -->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="Pulkit's Blog" />
        <meta name="twitter:description" content="Explore tutorials, projects, and insights into AI, coding, and more." />
        <meta name="twitter:image" content="img/social-preview.jpg" />
        
        
      
        <!-- Bootstrap CSS -->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
          rel="stylesheet"
        />
        <!-- Bootstrap Icons -->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css"
          rel="stylesheet"
        />
      
        <link rel="stylesheet" href="data/style.css?v=1.0.0" />
        <link
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"
        rel="stylesheet"
      />
      </head>
      <body>
        <div class="container mt-4">
          <div class="d-flex flex-column flex-md-row justify-content-between align-items-start align-items-md-center mb-4">
            <h2 class="fw-bold mb-3">Ridge Regression from Scratch</h2>
            <a href="../index.html" class="btn btn-primary">
              â† Back to Home
            </a>
          </div>
          <p class="text-muted">Published: <span id="publish-date">May 3, 2025</span></p>
          <hr />
        
          <h4 class="mt-4">ğŸ“Œ What is Regularization?</h4>
        <p>
            When building predictive models, itâ€™s tempting to chase perfection on the training data. But this can lead to <strong>overfitting</strong> â€” where your model memorizes noise instead of learning patterns.
            Thatâ€™s where <strong>regularization</strong> comes in.
        </p>
        <p>
            Regularization adds a penalty to the loss function to discourage large, unstable weights â€” nudging the model toward simpler, more generalizable solutions.
        </p>
        <p><strong>Benefits of Regularization:</strong></p>
        <ul>
            <li>Prevents overfitting</li>
            <li>Improves stability and generalization</li>
            <li>Especially helpful in high-dimensional or multicollinear data</li>
        </ul>

        
          <h4 class="mt-4">ğŸ“ Ridge Regression (L2)</h4>
          <p>
            Ridge regression adds a penalty to the squared magnitude of weights. This prevents the model from relying too heavily on any single feature:
          </p>
          <pre class="bg-light p-3 rounded"><code>Loss = MSE + Î± Ã— Î£(wáµ¢Â²)</code></pre>
          <ul>
            <li><strong>MSE</strong>: Mean Squared Error â€” the average squared difference between predicted and actual target values. This is your primary loss that the model tries to minimize.</li>
            <li><strong>Î± (alpha)</strong>: Regularization strength â€” a hyperparameter that controls how much penalty you apply to the weights. A higher Î± means more regularization (more shrinkage).</li>
            <li><strong>wáµ¢</strong>: Each individual weight/parameter (excluding the bias term). These are the model's coefficients.</li>
            <li><strong>Î£(wáµ¢Â²)</strong>: The sum of the squares of all weights. This is the L2 penalty â€” it encourages the model to keep weights small and prevents overfitting.</li>
          </ul>
          
          <p>
            So, Ridge regression doesnâ€™t just minimize prediction error â€” it also penalizes large weights. The goal is a balance between fitting the data well (<code>MSE</code>) and keeping the model simple (<code>Î± Ã— Î£(wáµ¢Â²)</code>).
          </p>
        
          <h4 class="mt-4">ğŸ  Load & Prepare the Data</h4>
          <pre class="bg-light p-3 rounded"><code>
        from sklearn.datasets import fetch_california_housing
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        
        housing = fetch_california_housing()
        X, y = housing.data, housing.target
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42)
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Add bias term
        import numpy as np
        X_train_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]
        X_test_bias = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]</code></pre>
        
          <h4 class="mt-4">ğŸ§  Ridge Regression (Gradient Descent Implementation)</h4>
          <pre class="bg-light p-3 rounded"><code>
        class RidgeRegressionGD:
            def __init__(self, alpha=1.0, learning_rate=0.01, n_iterations=1000):
                self.alpha = alpha
                self.learning_rate = learning_rate
                self.n_iterations = n_iterations
        
            def fit(self, X, y):
                m, n = X.shape
                self.weights = np.zeros(n)
                for _ in range(self.n_iterations):
                    predictions = X @ self.weights
                    errors = predictions - y
                    gradient = (1/m) * (X.T @ errors) + (2 * self.alpha / m) * np.r_[0, self.weights[1:]]
                    self.weights -= self.learning_rate * gradient
        
            def predict(self, X):
                return X @ self.weights</code></pre>
        
          <h4 class="mt-4">ğŸ“Š Train and Evaluate the Model</h4>
          <pre class="bg-light p-3 rounded"><code>
        from sklearn.metrics import mean_squared_error, r2_score
        
        ridge_gd = RidgeRegressionGD(alpha=1.0, learning_rate=0.1, n_iterations=1000)
        ridge_gd.fit(X_train_bias, y_train)
        preds = ridge_gd.predict(X_test_bias)
        
        mse = mean_squared_error(y_test, preds)
        r2 = r2_score(y_test, preds)
        
        print("ğŸ“‰ Ridge Regression (Gradient Descent):")
        print(f"MSE: {mse:.4f}")
        print(f"RÂ² Score: {r2:.4f}")</code></pre>
        <div class="mt-2 mb-4 text-center">
            <img src="../img/Blog_5_1.png" alt="Ridge Gradient Descent Results" class="img-fluid rounded shadow"/>
          </div>
          <h4 class="mt-4">âš–ï¸ Compare with OLS and SGD Regressor</h4>
        
          <h5>OLS (LinearRegression)</h5>
          <pre class="bg-light p-3 rounded"><code>
        from sklearn.linear_model import LinearRegression
        
        lr = LinearRegression()
        lr.fit(X_train_scaled, y_train)
        y_pred_lr = lr.predict(X_test_scaled)
        
        mse_lr = mean_squared_error(y_test, y_pred_lr)
        r2_lr = r2_score(y_test, y_pred_lr)</code></pre>
        <div class="mt-2 mb-4 text-center">
            <img src="../img/Blog_5_2.png" alt="OLS Linear Regression Results" class="img-fluid rounded shadow"/>
          </div>
          <h5>SGD Regressor (No Regularization)</h5>
          <pre class="bg-light p-3 rounded"><code>
        from sklearn.linear_model import SGDRegressor
        
        sgd = SGDRegressor(penalty=None, max_iter=1000, learning_rate='invscaling', eta0=0.01, random_state=42)
        sgd.fit(X_train_scaled, y_train)
        y_pred_sgd = sgd.predict(X_test_scaled)
        
        mse_sgd = mean_squared_error(y_test, y_pred_sgd)
        r2_sgd = r2_score(y_test, y_pred_sgd)</code></pre>
        <div class="mt-2 mb-4 text-center">
            <img src="../img/Blog_5_3.png" alt="SGD Regressor Results" class="img-fluid rounded shadow"/>
          </div>

          <h4 class="mt-4">ğŸ“ˆ Results Comparison</h4>
          <ul>
            <li><strong>OLS</strong>: MSE â‰ˆ <code>0.5559</code>, RÂ² â‰ˆ <code>0.5758</code></li>
            <li><strong>Ridge GD</strong>: MSE â‰ˆ <code>0.5559</code>, RÂ² â‰ˆ <code>0.5758</code></li>
            <li><strong>SGD</strong>: MSE â‰ˆ <code>0.5506</code>, RÂ² â‰ˆ <code>0.5798</code></li>
          </ul>
        
          <p>ğŸ¯ Ridge with gradient descent provides flexibility (custom learning schedules, mini-batching, etc.) while keeping regularization in play.</p>
        
          <h4 class="mt-4">ğŸ”— Reference</h4>
          <p><a href="https://github.com/Pulkit12dhingra/Blog/blob/main/notebooks/Ridge_Lasso_Regression.ipynb" target="_blank">
            ğŸ““ ridge_lasso_reg.ipynb on GitHub
          </a></p>
        </div>
        </body>
</html>
