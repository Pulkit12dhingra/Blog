
<div class="container mt-4">
    <h2 class="fw-bold mb-3">Unraveling Linear Regression with OLS â€” A Deep Dive Using California Housing Data ğŸ¡</h2>
    <p class="text-muted">Published: <span id="publish-date">April 19, 2025</span></p>
    <hr />

    <p>When you think about prediction in data science, linear regression is often the first technique that pops up. Itâ€™s simple, powerful, and forms the backbone of more complex machine learning models.</p>

    <p>In this blog, weâ€™ll explore <strong>Ordinary Least Squares (OLS)</strong> linear regression â€” not just by theory but by walking through a real-world dataset step-by-step. And what better way to do that than using the <strong>California Housing Dataset</strong> from <code>scikit-learn</code>?</p>

    <h4 class="mt-4">ğŸ§  What is OLS?</h4>
    <p>OLS is a mathematical method to determine the "best-fitting" line through a dataset. It works by minimizing the <strong>sum of squared errors</strong> between actual values and the values predicted by the model.</p>

    <p>Mathematically, the optimal weight vector \( \theta \) is calculated using:</p>
    <pre class="bg-light p-3 rounded"><code>Î¸ = (Xáµ€X)<sup>-1</sup>Xáµ€y</code></pre>



    <p>This is known as the <strong>Normal Equation</strong>. It avoids iteration (like in gradient descent) and directly gives us the best solution â€” if (Xáµ€X) is invertible.</p>

    
<h4 class="mt-4">ğŸ“Š Dataset Overview: California Housing</h4>
<p>For this tutorial, weâ€™re using the <strong>California Housing Dataset</strong> from scikit-learn. It's a classic regression dataset used for predicting the median house value across various districts in California.</p>

<p>ğŸ“Œ <strong>Dataset Details:</strong></p>
<ul>
  <li><strong>Type:</strong> Regression</li>
  <li><strong>Samples:</strong> 20,640</li>
  <li><strong>Features:</strong> 8 numerical features</li>
  <li><strong>Target:</strong> Median house value (in $100,000s)</li>
</ul>

<p>ğŸ” <strong>Features:</strong></p>
<ul>
  <li>longitude</li>
  <li>latitude</li>
  <li>housing_median_age</li>
  <li>total_rooms</li>
  <li>total_bedrooms</li>
  <li>population</li>
  <li>households</li>
  <li>median_income</li>
</ul>

<p>ğŸ¯ <strong>Target Variable:</strong> <code>median_house_value</code></p>


<h4 class="mt-4">ğŸ“¦ Step-by-Step Implementation</h4>

    <h5>1ï¸âƒ£ Load the California Housing Dataset</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.datasets import fetch_california_housing

housing = fetch_california_housing()
X = housing.data
y = housing.target
print("Data shape:", X.shape)
print("Target shape:", y.shape)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="../img/Blog_3/_1.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>
</div>


    <h5>2ï¸âƒ£ Split the Data into Train and Test Sets</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
    print("Training data shape:", X_train.shape)
    print("Testing data shape:", X_test.shape)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="../img/Blog_3/_2.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>
</div>


    <h5>3ï¸âƒ£ Add Bias Term (Intercept)</h5>
    <pre class="bg-light p-3 rounded"><code>import numpy as np

X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]
X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]</code></pre>



    <h5>4ï¸âƒ£ Apply the OLS Formula</h5>
    <pre class="bg-light p-3 rounded"><code>theta_best = np.linalg.inv(X_train_bias.T.dot(X_train_bias)).dot(X_train_bias.T).dot(y_train)</code></pre>



    <h5>5ï¸âƒ£ Make Predictions</h5>
    <pre class="bg-light p-3 rounded"><code>y_pred = X_test_bias.dot(theta_best)</code></pre>



    <h5>6ï¸âƒ£ Evaluate Model Performance</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error (manual OLS):", mse)
print("R^2 Score:", r2)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="../img/Blog_3/_3.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>
</div>


    <p>These metrics help us understand model accuracy. A higher RÂ² and lower MSE means a better fit.</p>

    <h4 class="mt-4">âš ï¸ What if (Xáµ€X) is Not Invertible?</h4>
    <p>If the matrix is singular (due to multicollinearity), we canâ€™t compute the inverse directly. This is where <strong>SVD (Singular Value Decomposition)</strong> helps:</p>
    <pre class="bg-light p-3 rounded"><code>from numpy.linalg import svd

U, s, Vt = svd(X_train_bias)</code></pre>


    <p>This numerically stable method still lets us solve OLS efficiently.</p>

    
<h4 class="mt-4">ğŸ¤– Comparing with scikit-learn's LinearRegression</h4>
<p>We can validate our manual OLS implementation by comparing it with scikit-learnâ€™s built-in <code>LinearRegression</code> model:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_sklearn_pred = model.predict(X_test)</code></pre>



<p>Then evaluate the performance using the same metrics:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse_sklearn = mean_squared_error(y_test, y_sklearn_pred)
r2_sklearn = r2_score(y_test, y_sklearn_pred)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="../img/Blog_3/_4.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>
</div>


<p>This allows you to confirm that both approaches yield nearly identical results â€” verifying that our OLS formula is working correctly!</p>
<p>ğŸ”¬ The comparison below shows the output from both models:</p>
<p><strong>Original (scikit-learn):</strong> MSE â‰ˆ 0.55589, RÂ² â‰ˆ 0.57578</p>
<p><strong>Manual OLS:</strong> MSE â‰ˆ 0.55589, RÂ² â‰ˆ 0.57578</p>

<p>This validates that our manual implementation produces the same result as scikit-learnâ€™s optimized solution.</p>

<div class="text-center">
  <img src="https://pulkit12dhingra.github.io/Blog/img/Blog_3/_5.png" alt="Model Comparison Results" class="img-fluid rounded shadow mt-3"/>
</div>



<h4 class="mt-4">âœ… Wrapping Up</h4>
    <p>OLS linear regression might look like just a formula â€” but when you apply it to a real dataset, each step teaches something new:</p>
    <ul>
        <li>Data preprocessing</li>
        <li>Matrix math</li>
        <li>Model evaluation</li>
        <li>Handling numerical instability</li>
    </ul>

    <p class="fw-bold">This is just the beginning. Weâ€™ll dive deeper into Ridge, Lasso, and more in upcoming blogs. Stay tuned! ğŸ™Œ</p>
</div>
