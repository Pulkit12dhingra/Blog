
<div class="container mt-4">
    <h2 class="fw-bold mb-3">Unraveling Linear Regression with OLS â€” A Deep Dive Using California Housing Data ğŸ¡</h2>
    <p class="text-muted">Published: <span id="publish-date">April 19, 2025</span></p>
    <hr />

    <p>When you think about prediction in data science, linear regression is often the first technique that pops up. Itâ€™s simple, powerful, and forms the backbone of more complex machine learning models.</p>

    <p>In this blog, weâ€™ll explore <strong>Ordinary Least Squares (OLS)</strong> linear regression â€” not just by theory but by walking through a real-world dataset step-by-step. And what better way to do that than using the <strong>California Housing Dataset</strong> from <code>scikit-learn</code>?</p>

    <h4 class="mt-4">ğŸ§  What is OLS?</h4>
    <p>OLS is a mathematical method to determine the "best-fitting" line through a dataset. It works by minimizing the <strong>sum of squared errors</strong> between actual values and the values predicted by the model.</p>

    <p>Mathematically, the optimal weight vector \( \theta \) is calculated using:</p>
    <pre class="bg-light p-3 rounded"><code>Î¸ = (Xáµ€X)<sup>-1</sup>Xáµ€y</code></pre>



    <p>This is known as the <strong>Normal Equation</strong>. It avoids iteration (like in gradient descent) and directly gives us the best solution â€” if (Xáµ€X) is invertible.</p>

    
<h4 class="mt-4">ğŸ“Š Dataset Overview: California Housing</h4>
<p>For this tutorial, weâ€™re using the <strong>California Housing Dataset</strong> from scikit-learn. It's a classic regression dataset used for predicting the median house value across various districts in California.</p>

<p>ğŸ“Œ <strong>Dataset Details:</strong></p>
<ul>
  <li><strong>Type:</strong> Regression</li>
  <li><strong>Samples:</strong> 20,640</li>
  <li><strong>Features:</strong> 8 numerical features</li>
  <li><strong>Target:</strong> Median house value (in $100,000s)</li>
</ul>

<p>ğŸ” <strong>Features:</strong></p>
<ul>
  <li>longitude</li>
  <li>latitude</li>
  <li>housing_median_age</li>
  <li>total_rooms</li>
  <li>total_bedrooms</li>
  <li>population</li>
  <li>households</li>
  <li>median_income</li>
</ul>

<p>ğŸ¯ <strong>Target Variable:</strong> <code>median_house_value</code></p>


<h4 class="mt-4">ğŸ“¦ Step-by-Step Implementation</h4>

    <h5>1ï¸âƒ£ Load the California Housing Dataset</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.datasets import fetch_california_housing

housing = fetch_california_housing()
X = housing.data
y = housing.target
print("Data shape:", X.shape)
print("Target shape:", y.shape)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="img/Blog_3_1.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>


</div>


    <h5>2ï¸âƒ£ Split the Data into Train and Test Sets</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
    print("Training data shape:", X_train.shape)
    print("Testing data shape:", X_test.shape)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="img/Blog_3_2.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>


</div>


    <h5>3ï¸âƒ£ Add Bias Term (Intercept)</h5>
    <pre class="bg-light p-3 rounded"><code>import numpy as np

X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]
X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]</code></pre>



    
<p><strong>Why add a bias term?</strong> In linear regression, the bias (or intercept) allows the model to fit data that doesn't necessarily pass through the origin (0,0). 
To include this, we prepend a column of 1s to the feature matrix using <code>np.ones()</code>. This new column represents the bias term, and its coefficient is learned just like the other feature weights.</p>


<h5>4ï¸âƒ£ Apply the OLS Formula</h5>
    <pre class="bg-light p-3 rounded"><code>theta_best = np.linalg.inv(X_train_bias.T.dot(X_train_bias)).dot(X_train_bias.T).dot(y_train)</code></pre>



    
<p><strong>What does this formula do?</strong> This is the classic <em>Normal Equation</em> for OLS regression:</p>

<ul>
  <li><code>X_train_bias.T.dot(X_train_bias)</code> computes the Gram matrix \( X^TX \)</li>
  <li><code>np.linalg.inv(...)</code> calculates its inverse â€” this step "undoes" the multiplication to isolate theta</li>
  <li><code>.dot(X_train_bias.T).dot(y_train)</code> completes the equation \( \theta = (X^TX)^{-1}X^Ty \)</li>
</ul>

<p>This yields the exact weight vector <code>theta_best</code> that minimizes the squared error between predictions and true values â€” a closed-form solution!</p>
<p><strong>Note:</strong> If the matrix is non-invertible (singular), this will throw an error â€” thatâ€™s when SVD or regularized methods come in handy.</p>


<h5>5ï¸âƒ£ Make Predictions</h5>
    <pre class="bg-light p-3 rounded"><code>y_pred = X_test_bias.dot(theta_best)</code></pre>



    <h5>6ï¸âƒ£ Evaluate Model Performance</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error (manual OLS):", mse)
print("R^2 Score:", r2)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="img/Blog_3_3.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>


</div>


    <p>These metrics help us understand model accuracy. A higher RÂ² and lower MSE means a better fit.</p>

    <h4 class="mt-4">âš ï¸ What if (Xáµ€X) is Not Invertible?</h4>
    <p>If the matrix is singular (due to multicollinearity), we canâ€™t compute the inverse directly. This is where <strong>SVD (Singular Value Decomposition)</strong> helps:</p>
    <pre class="bg-light p-3 rounded"><code>from numpy.linalg import svd

U, s, Vt = svd(X_train_bias)</code></pre>


    <p>This numerically stable method still lets us solve OLS efficiently.</p>
<p><strong>What is SVD (Singular Value Decomposition)?</strong></p>
<p>Singular Value Decomposition (SVD) is a powerful matrix factorization technique used to decompose a matrix into three other matrices:</p>

<pre class="bg-light p-3 rounded"><code>from numpy.linalg import svd

U, s, Vt = svd(X_train_bias)</code></pre>

<p>Hereâ€™s what each component represents:</p>

<ul>
  <li><code>U</code>: An orthogonal matrix of shape (m Ã— m), where m is the number of training samples. It contains the left singular vectors.</li>
  <li><code>s</code>: A 1D array of singular values (diagonal elements of Î£). These are always non-negative and sorted in descending order.</li>
  <li><code>Vt</code>: The transpose of an orthogonal matrix V (of shape n Ã— n), where n is the number of features (including the bias column). It contains the right singular vectors.</li>
</ul>

<p>Together, these satisfy the identity:</p>

<pre><code>X â‰ˆ U Ã— Î£ Ã— Váµ—</code></pre>

<p>Using SVD is especially useful when the matrix <code>Xáµ€X</code> is not invertible or poorly conditioned. Instead of relying on inversion, SVD allows for a more numerically stable way to solve for the weights in linear regression.</p>


    
<h4 class="mt-4">ğŸ¤– Comparing with scikit-learn's LinearRegression</h4>
<p>We can validate our manual OLS implementation by comparing it with scikit-learnâ€™s built-in <code>LinearRegression</code> model:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_sklearn_pred = model.predict(X_test)</code></pre>



<p>Then evaluate the performance using the same metrics:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse_sklearn = mean_squared_error(y_test, y_sklearn_pred)
r2_sklearn = r2_score(y_test, y_sklearn_pred)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="img/Blog_3_4.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>


</div>


<p>This allows you to confirm that both approaches yield nearly identical results â€” verifying that our OLS formula is working correctly!</p>
<p>ğŸ”¬ The comparison below shows the output from both models:</p>
<p><strong>Original (scikit-learn):</strong> MSE â‰ˆ 0.55589, RÂ² â‰ˆ 0.57578</p>
<p><strong>Manual OLS:</strong> MSE â‰ˆ 0.55589, RÂ² â‰ˆ 0.57578</p>

<p>This validates that our manual implementation produces the same result as scikit-learnâ€™s optimized solution.</p>

<div class="text-center">
  <img src="img/Blog_3_5.png" alt="Model Comparison Results" class="img-fluid rounded shadow mt-3"/>


</div>



<h4 class="mt-4">âœ… Wrapping Up</h4>
    <p>OLS linear regression might look like just a formula â€” but when you apply it to a real dataset, each step teaches something new:</p>
    <ul>
        <li>Data preprocessing</li>
        <li>Matrix math</li>
        <li>Model evaluation</li>
        <li>Handling numerical instability</li>
    </ul>

    <p class="fw-bold">This is just the beginning. Weâ€™ll dive deeper into Ridge, Lasso, and more in upcoming blogs. Stay tuned! ğŸ™Œ</p>

<h4 class="mt-4">ğŸ”— Reference</h4>
<p>You can explore the full notebook used to generate this blog on GitHub:</p>
<p><a href="https://github.com/Pulkit12dhingra/Blog/blob/main/notebooks/Linear_Regression_OLS.ipynb" target="_blank">
  ğŸ““ Linear_Regression_OLS.ipynb on GitHub
</a></p>

</div>
