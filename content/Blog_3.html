
<div class="container mt-4">
    <h2 class="fw-bold mb-3">Unraveling Linear Regression with OLS — A Deep Dive Using California Housing Data 🏡</h2>
    <p class="text-muted">Published: <span id="publish-date">April 19, 2025</span></p>
    <hr />

    <p>When you think about prediction in data science, linear regression is often the first technique that pops up. It’s simple, powerful, and forms the backbone of more complex machine learning models.</p>

    <p>In this blog, we’ll explore <strong>Ordinary Least Squares (OLS)</strong> linear regression — not just by theory but by walking through a real-world dataset step-by-step. And what better way to do that than using the <strong>California Housing Dataset</strong> from <code>scikit-learn</code>?</p>

    <h4 class="mt-4">🧠 What is OLS?</h4>
    <p>OLS is a mathematical method to determine the "best-fitting" line through a dataset. It works by minimizing the <strong>sum of squared errors</strong> between actual values and the values predicted by the model.</p>

    <p>Mathematically, the optimal weight vector \( \theta \) is calculated using:</p>
    <pre class="bg-light p-3 rounded"><code>θ = (XᵀX)<sup>-1</sup>Xᵀy</code></pre>



    <p>This is known as the <strong>Normal Equation</strong>. It avoids iteration (like in gradient descent) and directly gives us the best solution — if (XᵀX) is invertible.</p>

    
<h4 class="mt-4">📊 Dataset Overview: California Housing</h4>
<p>For this tutorial, we’re using the <strong>California Housing Dataset</strong> from scikit-learn. It's a classic regression dataset used for predicting the median house value across various districts in California.</p>

<p>📌 <strong>Dataset Details:</strong></p>
<ul>
  <li><strong>Type:</strong> Regression</li>
  <li><strong>Samples:</strong> 20,640</li>
  <li><strong>Features:</strong> 8 numerical features</li>
  <li><strong>Target:</strong> Median house value (in $100,000s)</li>
</ul>

<p>🔍 <strong>Features:</strong></p>
<ul>
  <li>longitude</li>
  <li>latitude</li>
  <li>housing_median_age</li>
  <li>total_rooms</li>
  <li>total_bedrooms</li>
  <li>population</li>
  <li>households</li>
  <li>median_income</li>
</ul>

<p>🎯 <strong>Target Variable:</strong> <code>median_house_value</code></p>


<h4 class="mt-4">📦 Step-by-Step Implementation</h4>

    <h5>1️⃣ Load the California Housing Dataset</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.datasets import fetch_california_housing

housing = fetch_california_housing()
X = housing.data
y = housing.target
print("Data shape:", X.shape)
print("Target shape:", y.shape)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="img/Blog_3_1.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>


</div>


    <h5>2️⃣ Split the Data into Train and Test Sets</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
    print("Training data shape:", X_train.shape)
    print("Testing data shape:", X_test.shape)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="img/Blog_3_2.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>


</div>


    <h5>3️⃣ Add Bias Term (Intercept)</h5>
    <pre class="bg-light p-3 rounded"><code>import numpy as np

X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]
X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]</code></pre>



    
<p><strong>Why add a bias term?</strong> In linear regression, the bias (or intercept) allows the model to fit data that doesn't necessarily pass through the origin (0,0). 
To include this, we prepend a column of 1s to the feature matrix using <code>np.ones()</code>. This new column represents the bias term, and its coefficient is learned just like the other feature weights.</p>


<h5>4️⃣ Apply the OLS Formula</h5>
    <pre class="bg-light p-3 rounded"><code>theta_best = np.linalg.inv(X_train_bias.T.dot(X_train_bias)).dot(X_train_bias.T).dot(y_train)</code></pre>



    
<p><strong>What does this formula do?</strong> This is the classic <em>Normal Equation</em> for OLS regression:</p>

<ul>
  <li><code>X_train_bias.T.dot(X_train_bias)</code> computes the Gram matrix \( X^TX \)</li>
  <li><code>np.linalg.inv(...)</code> calculates its inverse — this step "undoes" the multiplication to isolate theta</li>
  <li><code>.dot(X_train_bias.T).dot(y_train)</code> completes the equation \( \theta = (X^TX)^{-1}X^Ty \)</li>
</ul>

<p>This yields the exact weight vector <code>theta_best</code> that minimizes the squared error between predictions and true values — a closed-form solution!</p>
<p><strong>Note:</strong> If the matrix is non-invertible (singular), this will throw an error — that’s when SVD or regularized methods come in handy.</p>


<h5>5️⃣ Make Predictions</h5>
    <pre class="bg-light p-3 rounded"><code>y_pred = X_test_bias.dot(theta_best)</code></pre>



    <h5>6️⃣ Evaluate Model Performance</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error (manual OLS):", mse)
print("R^2 Score:", r2)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="img/Blog_3_3.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>


</div>


    <p>These metrics help us understand model accuracy. A higher R² and lower MSE means a better fit.</p>

    <h4 class="mt-4">⚠️ What if (XᵀX) is Not Invertible?</h4>
    <p>If the matrix is singular (due to multicollinearity), we can’t compute the inverse directly. This is where <strong>SVD (Singular Value Decomposition)</strong> helps:</p>
    <pre class="bg-light p-3 rounded"><code>from numpy.linalg import svd

U, s, Vt = svd(X_train_bias)</code></pre>


    <p>This numerically stable method still lets us solve OLS efficiently.</p>
<p><strong>What is SVD (Singular Value Decomposition)?</strong></p>
<p>Singular Value Decomposition (SVD) is a powerful matrix factorization technique used to decompose a matrix into three other matrices:</p>

<pre class="bg-light p-3 rounded"><code>from numpy.linalg import svd

U, s, Vt = svd(X_train_bias)</code></pre>

<p>Here’s what each component represents:</p>

<ul>
  <li><code>U</code>: An orthogonal matrix of shape (m × m), where m is the number of training samples. It contains the left singular vectors.</li>
  <li><code>s</code>: A 1D array of singular values (diagonal elements of Σ). These are always non-negative and sorted in descending order.</li>
  <li><code>Vt</code>: The transpose of an orthogonal matrix V (of shape n × n), where n is the number of features (including the bias column). It contains the right singular vectors.</li>
</ul>

<p>Together, these satisfy the identity:</p>

<pre><code>X ≈ U × Σ × Vᵗ</code></pre>

<p>Using SVD is especially useful when the matrix <code>XᵀX</code> is not invertible or poorly conditioned. Instead of relying on inversion, SVD allows for a more numerically stable way to solve for the weights in linear regression.</p>


    
<h4 class="mt-4">🤖 Comparing with scikit-learn's LinearRegression</h4>
<p>We can validate our manual OLS implementation by comparing it with scikit-learn’s built-in <code>LinearRegression</code> model:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_sklearn_pred = model.predict(X_test)</code></pre>



<p>Then evaluate the performance using the same metrics:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse_sklearn = mean_squared_error(y_test, y_sklearn_pred)
r2_sklearn = r2_score(y_test, y_sklearn_pred)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="img/Blog_3_4.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>


</div>


<p>This allows you to confirm that both approaches yield nearly identical results — verifying that our OLS formula is working correctly!</p>
<p>🔬 The comparison below shows the output from both models:</p>
<p><strong>Original (scikit-learn):</strong> MSE ≈ 0.55589, R² ≈ 0.57578</p>
<p><strong>Manual OLS:</strong> MSE ≈ 0.55589, R² ≈ 0.57578</p>

<p>This validates that our manual implementation produces the same result as scikit-learn’s optimized solution.</p>

<div class="text-center">
  <img src="img/Blog_3_5.png" alt="Model Comparison Results" class="img-fluid rounded shadow mt-3"/>


</div>



<h4 class="mt-4">✅ Wrapping Up</h4>
    <p>OLS linear regression might look like just a formula — but when you apply it to a real dataset, each step teaches something new:</p>
    <ul>
        <li>Data preprocessing</li>
        <li>Matrix math</li>
        <li>Model evaluation</li>
        <li>Handling numerical instability</li>
    </ul>

    <p class="fw-bold">This is just the beginning. We’ll dive deeper into Ridge, Lasso, and more in upcoming blogs. Stay tuned! 🙌</p>

<h4 class="mt-4">🔗 Reference</h4>
<p>You can explore the full notebook used to generate this blog on GitHub:</p>
<p><a href="https://github.com/Pulkit12dhingra/Blog/blob/main/notebooks/Linear_Regression_OLS.ipynb" target="_blank">
  📓 Linear_Regression_OLS.ipynb on GitHub
</a></p>

</div>
