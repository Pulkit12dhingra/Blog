
<div class="container mt-4">
    <h2 class="fw-bold mb-3">Unraveling Linear Regression with OLS — A Deep Dive Using California Housing Data 🏡</h2>
    <p class="text-muted">Published: <span id="publish-date">April 19, 2025</span></p>
    <hr />

    <p>When you think about prediction in data science, linear regression is often the first technique that pops up. It’s simple, powerful, and forms the backbone of more complex machine learning models.</p>

    <p>In this blog, we’ll explore <strong>Ordinary Least Squares (OLS)</strong> linear regression — not just by theory but by walking through a real-world dataset step-by-step. And what better way to do that than using the <strong>California Housing Dataset</strong> from <code>scikit-learn</code>?</p>

    <h4 class="mt-4">🧠 What is OLS?</h4>
    <p>OLS is a mathematical method to determine the "best-fitting" line through a dataset. It works by minimizing the <strong>sum of squared errors</strong> between actual values and the values predicted by the model.</p>

    <p>Mathematically, the optimal weight vector \( \theta \) is calculated using:</p>
    <pre class="bg-light p-3 rounded"><code>θ = (XᵀX)<sup>-1</sup>Xᵀy</code></pre>



    <p>This is known as the <strong>Normal Equation</strong>. It avoids iteration (like in gradient descent) and directly gives us the best solution — if (XᵀX) is invertible.</p>

    
<h4 class="mt-4">📊 Dataset Overview: California Housing</h4>
<p>For this tutorial, we’re using the <strong>California Housing Dataset</strong> from scikit-learn. It's a classic regression dataset used for predicting the median house value across various districts in California.</p>

<p>📌 <strong>Dataset Details:</strong></p>
<ul>
  <li><strong>Type:</strong> Regression</li>
  <li><strong>Samples:</strong> 20,640</li>
  <li><strong>Features:</strong> 8 numerical features</li>
  <li><strong>Target:</strong> Median house value (in $100,000s)</li>
</ul>

<p>🔍 <strong>Features:</strong></p>
<ul>
  <li>longitude</li>
  <li>latitude</li>
  <li>housing_median_age</li>
  <li>total_rooms</li>
  <li>total_bedrooms</li>
  <li>population</li>
  <li>households</li>
  <li>median_income</li>
</ul>

<p>🎯 <strong>Target Variable:</strong> <code>median_house_value</code></p>


<h4 class="mt-4">📦 Step-by-Step Implementation</h4>

    <h5>1️⃣ Load the California Housing Dataset</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.datasets import fetch_california_housing

housing = fetch_california_housing()
X = housing.data
y = housing.target
print("Data shape:", X.shape)
print("Target shape:", y.shape)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="../img/Blog_3/_1.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>
</div>


    <h5>2️⃣ Split the Data into Train and Test Sets</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
    print("Training data shape:", X_train.shape)
    print("Testing data shape:", X_test.shape)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="../img/Blog_3/_2.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>
</div>


    <h5>3️⃣ Add Bias Term (Intercept)</h5>
    <pre class="bg-light p-3 rounded"><code>import numpy as np

X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]
X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]</code></pre>



    <h5>4️⃣ Apply the OLS Formula</h5>
    <pre class="bg-light p-3 rounded"><code>theta_best = np.linalg.inv(X_train_bias.T.dot(X_train_bias)).dot(X_train_bias.T).dot(y_train)</code></pre>



    <h5>5️⃣ Make Predictions</h5>
    <pre class="bg-light p-3 rounded"><code>y_pred = X_test_bias.dot(theta_best)</code></pre>



    <h5>6️⃣ Evaluate Model Performance</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error (manual OLS):", mse)
print("R^2 Score:", r2)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="../img/Blog_3/_3.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>
</div>


    <p>These metrics help us understand model accuracy. A higher R² and lower MSE means a better fit.</p>

    <h4 class="mt-4">⚠️ What if (XᵀX) is Not Invertible?</h4>
    <p>If the matrix is singular (due to multicollinearity), we can’t compute the inverse directly. This is where <strong>SVD (Singular Value Decomposition)</strong> helps:</p>
    <pre class="bg-light p-3 rounded"><code>from numpy.linalg import svd

U, s, Vt = svd(X_train_bias)</code></pre>


    <p>This numerically stable method still lets us solve OLS efficiently.</p>

    
<h4 class="mt-4">🤖 Comparing with scikit-learn's LinearRegression</h4>
<p>We can validate our manual OLS implementation by comparing it with scikit-learn’s built-in <code>LinearRegression</code> model:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_sklearn_pred = model.predict(X_test)</code></pre>



<p>Then evaluate the performance using the same metrics:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse_sklearn = mean_squared_error(y_test, y_sklearn_pred)
r2_sklearn = r2_score(y_test, y_sklearn_pred)</code></pre>
<div class="mt-2 mb-4 text-center">
  <img src="../img/Blog_3/_4.png" alt="Code Output Screenshot" class="img-fluid rounded shadow"/>
</div>


<p>This allows you to confirm that both approaches yield nearly identical results — verifying that our OLS formula is working correctly!</p>
<p>🔬 The comparison below shows the output from both models:</p>
<p><strong>Original (scikit-learn):</strong> MSE ≈ 0.55589, R² ≈ 0.57578</p>
<p><strong>Manual OLS:</strong> MSE ≈ 0.55589, R² ≈ 0.57578</p>

<p>This validates that our manual implementation produces the same result as scikit-learn’s optimized solution.</p>

<div class="text-center">
  <img src="https://pulkit12dhingra.github.io/Blog/img/Blog_3/_5.png" alt="Model Comparison Results" class="img-fluid rounded shadow mt-3"/>
</div>



<h4 class="mt-4">✅ Wrapping Up</h4>
    <p>OLS linear regression might look like just a formula — but when you apply it to a real dataset, each step teaches something new:</p>
    <ul>
        <li>Data preprocessing</li>
        <li>Matrix math</li>
        <li>Model evaluation</li>
        <li>Handling numerical instability</li>
    </ul>

    <p class="fw-bold">This is just the beginning. We’ll dive deeper into Ridge, Lasso, and more in upcoming blogs. Stay tuned! 🙌</p>
</div>
