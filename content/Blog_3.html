
<div class="container mt-4">
    <h2 class="fw-bold mb-3">Unraveling Linear Regression with OLS ‚Äî A Deep Dive Using California Housing Data üè°</h2>
    <p class="text-muted">Published: <span id="publish-date">April 19, 2025</span></p>
    <hr />

    <p>When you think about prediction in data science, linear regression is often the first technique that pops up. It‚Äôs simple, powerful, and forms the backbone of more complex machine learning models.</p>

    <p>In this blog, we‚Äôll explore <strong>Ordinary Least Squares (OLS)</strong> linear regression ‚Äî not just by theory but by walking through a real-world dataset step-by-step. And what better way to do that than using the <strong>California Housing Dataset</strong> from <code>scikit-learn</code>?</p>

    <h4 class="mt-4">üß† What is OLS?</h4>
    <p>OLS is a mathematical method to determine the "best-fitting" line through a dataset. It works by minimizing the <strong>sum of squared errors</strong> between actual values and the values predicted by the model.</p>

    <p>Mathematically, the optimal weight vector \( \theta \) is calculated using:</p>
    <pre class="bg-light p-3 rounded"><code>Œ∏ = (X·µÄX)<sup>-1</sup>X·µÄy</code></pre>

    <p>This is known as the <strong>Normal Equation</strong>. It avoids iteration (like in gradient descent) and directly gives us the best solution ‚Äî if (X·µÄX) is invertible.</p>

    <h4 class="mt-4">üì¶ Step-by-Step Implementation</h4>

    <h5>1Ô∏è‚É£ Load the California Housing Dataset</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.datasets import fetch_california_housing

housing = fetch_california_housing()
X = housing.data
y = housing.target</code></pre>

    <h5>2Ô∏è‚É£ Split the Data into Train and Test Sets</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)</code></pre>

    <h5>3Ô∏è‚É£ Add Bias Term (Intercept)</h5>
    <pre class="bg-light p-3 rounded"><code>import numpy as np

X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]
X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]</code></pre>

    <h5>4Ô∏è‚É£ Apply the OLS Formula</h5>
    <pre class="bg-light p-3 rounded"><code>theta_best = np.linalg.inv(X_train_bias.T.dot(X_train_bias)).dot(X_train_bias.T).dot(y_train)</code></pre>

    <h5>5Ô∏è‚É£ Make Predictions</h5>
    <pre class="bg-light p-3 rounded"><code>y_pred = X_test_bias.dot(theta_best)</code></pre>

    <h5>6Ô∏è‚É£ Evaluate Model Performance</h5>
    <pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)</code></pre>

    <p>These metrics help us understand model accuracy. A higher R¬≤ and lower MSE means a better fit.</p>

    <h4 class="mt-4">‚ö†Ô∏è What if (X·µÄX) is Not Invertible?</h4>
    <p>If the matrix is singular (due to multicollinearity), we can‚Äôt compute the inverse directly. This is where <strong>SVD (Singular Value Decomposition)</strong> helps:</p>
    <pre class="bg-light p-3 rounded"><code>from numpy.linalg import svd

U, s, Vt = svd(X_train_bias)</code></pre>

    <p>This numerically stable method still lets us solve OLS efficiently.</p>

    
<h4 class="mt-4">ü§ñ Comparing with scikit-learn's LinearRegression</h4>
<p>We can validate our manual OLS implementation by comparing it with scikit-learn‚Äôs built-in <code>LinearRegression</code> model:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_sklearn_pred = model.predict(X_test)</code></pre>

<p>Then evaluate the performance using the same metrics:</p>

<pre class="bg-light p-3 rounded"><code>from sklearn.metrics import mean_squared_error, r2_score

mse_sklearn = mean_squared_error(y_test, y_sklearn_pred)
r2_sklearn = r2_score(y_test, y_sklearn_pred)</code></pre>

<p>This allows you to confirm that both approaches yield nearly identical results ‚Äî verifying that our OLS formula is working correctly!</p>


<h4 class="mt-4">‚úÖ Wrapping Up</h4>
    <p>OLS linear regression might look like just a formula ‚Äî but when you apply it to a real dataset, each step teaches something new:</p>
    <ul>
        <li>Data preprocessing</li>
        <li>Matrix math</li>
        <li>Model evaluation</li>
        <li>Handling numerical instability</li>
    </ul>

    <p class="fw-bold">This is just the beginning. We‚Äôll dive deeper into Ridge, Lasso, and more in upcoming blogs. Stay tuned! üôå</p>
</div>
