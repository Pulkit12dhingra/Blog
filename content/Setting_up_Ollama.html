<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="Get started with Ollama, a no-token, local-friendly way to run LLMs on your machine. Setup steps and essential commands included." />
  <title>Setting Up Ollama | Byte-Sized-Brilliance-AI</title>
  <link rel="icon" type="image/png" href="../img/icon.png" />

  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="preconnect" href="https://www.googletagmanager.com" crossorigin>

  <!-- Open Graph -->
  <meta property="og:title" content="Setting Up Ollama" />
  <meta property="og:description" content="Get started with Ollama, a no-token, local-friendly way to run LLMs on your machine. Setup steps and essential commands included." />
  <meta property="og:image" content="../img/social-preview.jpg" />
  <meta property="og:type" content="article" />

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Setting Up Ollama" />
  <meta name="twitter:description" content="Get started with Ollama, a no-token, local-friendly way to run LLMs on your machine. Setup steps and essential commands included." />
  <meta name="twitter:image" content="../img/social-preview.jpg" />

  <!-- Critical CSS loaded first -->
  <link rel="stylesheet" href="../data/style.css?v=1.0.0" />

  <!-- Non-critical CSS loaded asynchronously -->
  <link
    href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
    rel="stylesheet"
    media="print"
    onload="this.media='all'"
  />
  <noscript>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" />
  </noscript>

  <link
    href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css"
    rel="stylesheet"
    media="print"
    onload="this.media='all'"
  />
  <noscript>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet" />
  </noscript>

  <link
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"
    rel="stylesheet"
    media="print"
    onload="this.media='all'"
  />
  <noscript>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" rel="stylesheet" />
  </noscript>

  <!-- Deferred Google Analytics -->
  <script>
    window.addEventListener('load', function() {
      setTimeout(function() {
        var script = document.createElement('script');
        script.src = 'https://www.googletagmanager.com/gtag/js?id=G-8JMS98M7C7';
        script.async = true;
        script.onload = function() {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-8JMS98M7C7');
        };
        document.head.appendChild(script);
      }, 1500);
    });
  </script>
</head>

<body>
  <div class="container mt-4">
    <div class="d-flex flex-column flex-md-row justify-content-between align-items-start align-items-md-center mb-4">
      <h2 class="fw-bold mb-3">Setting Up Ollama </h2>
    <a href="../index.html" class="btn btn-primary">‚Üê Back to Home</a>
    </div>
    <hr />

    <h4 class="mt-4">ü§î What is Ollama?</h4>
    <p>
      <strong>Ollama</strong> is a lightweight, local LLM runtime that lets you run models like LLaMA, Mistral, and Gemma directly on your machine ‚Äî without needing a GPU or even an API token. 
      It‚Äôs perfect for experimentation, prototyping, and even production-grade setups with privacy in mind.
    </p>
    <p>
      Think of it as the ‚ÄúDocker for language models‚Äù ‚Äî download a model once and run it anywhere, completely offline.
    </p>

    <h4 class="mt-4"> Why Ollama?</h4>
    <ul>
      <li>No API keys or accounts needed</li>
      <li>Run popular open-source models locally</li>
      <li>Easy-to-use CLI</li>
      <li>Great for testing ideas without cloud dependencies</li>
    </ul>

    <h4 class="mt-4"> Downloading and Installing Ollama</h4>
    <p>You can grab the latest release from the official website: <a href="https://ollama.com/" target="_blank">ollama.com</a>.</p>
    <ol>
      <li>Visit <code>https://ollama.com</code></li>
      <li>Click on <strong>Download</strong></li>
      <li>Select your operating system (macOS, Windows, or Linux)</li>
      <li>Once downloaded, run the installer</li>
      <li>Click through the setup steps (Next ‚Üí Install)</li>
      <li>After installation, open your terminal/command prompt</li>
      <li>Run: <code>ollama --help</code> to verify it‚Äôs working</li>
    </ol>

    <h4 class="mt-4"> Basic Commands</h4>

    <h5>üì• Pull a model</h5>
    <pre class="bg-light p-3 rounded"><code>ollama pull gemma3</code></pre>
    <p>This will download the latest version of the specified model.</p>
    <div class="mt-2 mb-4 text-center">
        <img src="../img/Blog_8_1.png" alt="ollama pull" class="img-fluid rounded shadow"/>
    </div>

    <h5> Run a model</h5>
    <pre class="bg-light p-3 rounded"><code>ollama run gemma3</code></pre>
    <p>Starts an interactive session with the model in your terminal.</p>
    <div class="mt-2 mb-4 text-center">
        <img src="../img/Blog_8_2.png" alt="ollama run" class="img-fluid rounded shadow"/>
    </div>

    <h5>üìã List installed models</h5>
    <pre class="bg-light p-3 rounded"><code>ollama list</code></pre>
    <div class="mt-2 mb-4 text-center">
        <img src="../img/Blog_8_3.png" alt="ollama list" class="img-fluid rounded shadow"/>
    </div>

    <h5> Remove a model</h5>
    <pre class="bg-light p-3 rounded"><code>ollama rm llama2</code></pre>
    
    <h5> View model info</h5>
    <pre class="bg-light p-3 rounded"><code>ollama show gemma3</code></pre>
    <div class="mt-2 mb-4 text-center">
        <img src="../img/Blog_8_4.png" alt="ollama show" class="img-fluid rounded shadow"/>
    </div>

    <h5> Start the server manually</h5>
    <pre class="bg-light p-3 rounded"><code>ollama serve</code></pre>

    <h5> Stop the running model</h5>
    <pre class="bg-light p-3 rounded"><code>ollama stop</code></pre>

    <h4 class="mt-4">üìé Final Thoughts</h4>
    <p>
      Ollama is a fantastic tool if you want local inference without the hassle of API keys or cloud latency. It‚Äôs fast, private, and incredibly easy to use. With just a few commands, you‚Äôll have state-of-the-art models running right on your machine.
    </p>
    <p class="fw-bold">Give it a try, and see how it fits into your AI workflow!</p>

    <h4 class="mt-4"> Reference</h4>
    <p><a href="https://ollama.com/" target="_blank">ollama.com</a></p>
  </div>

<!-- Footer -->
<footer class="bg-light py-4 mt-5 border-top">
  <div class="container text-center">
    <p class="text-muted mb-1">
      <i class="bi bi-person-circle me-2"></i>
      <strong>Written by:</strong> <a href="https://www.linkedin.com/in/pulkit-dhingra-4b7312193/" target="_blank" rel="noopener noreferrer" class="text-decoration-none">Pulkit Dhingra</a>
    </p>
    <p class="text-muted small mb-0">
      ¬© 2025 Byte-Sized-Brilliance-AI. All rights reserved.
    </p>
  </div>
</footer>

</body>
</html>
