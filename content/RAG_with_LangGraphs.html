<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Byte-Sized-Brilliance-AI</title>
  <link rel="icon" type="image/png" href="../img/icon.png" />

  <!-- Meta for sharing -->
  <meta property="og:title" content="Pulkit's Blog" />
  <meta property="og:description" content="Build a Retrieval-Augmented Generation (RAG) pipeline using PDF documents, Chroma DB, and Gemma 3." />
  <meta property="og:image" content="../img/social-preview.jpg" />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Pulkit's Blog" />
  <meta name="twitter:description" content="Build a Retrieval-Augmented Generation (RAG) pipeline using PDF documents, Chroma DB, and Gemma 3." />
  <meta name="twitter:image" content="../img/social-preview.jpg" />

  <!-- CSS and Bootstrap -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" />
  <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet" />
  <link rel="stylesheet" href="data/style.css?v=1.0.0" />
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" rel="stylesheet"/>
</head>
<body>
<div class="container mt-4">
  <div class="d-flex flex-column flex-md-row justify-content-between align-items-start align-items-md-center mb-4">
    <h2 class="fw-bold mb-3">Building a RAG Pipeline with PDFs, Chroma DB, and Gemma 3 </h2>
    <a href="../index.html" class="btn btn-primary">‚Üê Back to Home</a>
  </div>
  <hr />
  <p>
    <strong> This blog builds on:</strong> 
    <a href="https://pulkit12dhingra.github.io/Blog/content/Building_a_RAG_Pipeline_with_PDFs.html" target="_blank">
      Building a RAG Pipeline with PDFs (Original Tutorial)
    </a>
  </p>

  

  <h4 class="mt-5"> Part 2: RAG with LangGraph ‚Äî Making Pipelines Smarter</h4>
  <p>
    In this continuation, we move beyond the linear RAG setup by introducing <strong>LangGraph</strong>: a powerful orchestration library built for LLM apps. Think of it as the glue that connects steps like <code>retrieve</code> and <code>generate</code> into a dynamic, state-driven flow.
  </p>

  <h5 class="mt-4">üß© Why LangGraph?</h5>
  <ul>
    <li>Define each pipeline step as a node (function)</li>
    <li>Maintain shared <code>state</code> across steps</li>
    <li>Visualize and control application logic like a graph</li>
  </ul>

  <hr class="my-4"/>

  <h5 class="mt-4">üì• Load the PDFs</h5>
  <p> Let's start by loading in the pdf dataset using langchains. <code>document_loaders</code> extract text 
    from every PDF file abstracting away the complexity of parsing individual files.</p>
  <pre class="bg-light p-3 rounded"><code>
from langchain.document_loaders import PyPDFLoader
import os

pdf_folder = "data/pdfs"
all_docs = []

for filename in os.listdir(pdf_folder):
    if filename.endswith(".pdf"):
        loader = PyPDFLoader(os.path.join(pdf_folder, filename))
        all_docs.extend(loader.load())</code></pre>

  <h5 class="mt-4"> Split Text into Chunks</h5>
<p>
  In this step, we prepare the raw text for semantic search and generation by <strong>splitting it into manageable chunks</strong>. 
  Language models like Gemma 3 have context length limits, so feeding them an entire PDF at once isn't practical.
</p>
<p>
  We use LangChain‚Äôs <code>RecursiveCharacterTextSplitter</code>, which intelligently splits text into overlapping chunks 
  of up to <code>1000</code> characters each, with a <code>200</code>-character overlap. This overlap helps maintain context across chunk boundaries,
  ensuring that no important information is lost when slicing the document.
</p>
<p>
  The resulting <code>chunks</code> list contains smaller document segments that are suitable for vector embedding, retrieval, and input to the LLM.
</p>

  <pre class="bg-light p-3 rounded"><code>
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(all_docs)</code></pre>

  <h5 class="mt-4"> Define the Embeddings</h5>
  <p>
  This step converts each chunk of text into a numerical vector using a pretrained embedding model. 
  We use the <code>HuggingFaceEmbeddings</code> wrapper with the <code>all-MiniLM-L6-v2</code> model ‚Äî a lightweight and efficient transformer from the 
  <a href="https://www.sbert.net/" target="_blank">SentenceTransformers</a> library.
</p>
<p>
  Each vector captures the semantic meaning of its corresponding chunk. These embeddings are crucial for performing <strong>similarity search</strong> in the vector database later on. 
  When a user asks a question, we‚Äôll compare it against these vectors to retrieve the most relevant pieces of text.
</p>
<p>
  This setup enables our RAG pipeline to reason over large documents by mapping both queries and document chunks into the same embedding space.
</p>

  <pre class="bg-light p-3 rounded"><code>
from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")</code></pre>

  <h5 class="mt-4">üíæ Store Chunks in Chroma DB</h5>
  <p>
  Now that each text chunk has been converted into a vector, we store them in a vector database ‚Äî in this case, <strong>Chroma DB</strong>. 
  A vector database allows us to perform fast and efficient <strong>similarity searches</strong> based on the embeddings.
</p>
<p>
  The function <code>Chroma.from_documents(...)</code> takes in our list of chunks, embeds them using the model we defined earlier, and saves them in a local directory (<code>./chroma_db</code>) for persistence.
</p>
<p>
  This vectorstore acts as the memory for our RAG system. When a user asks a question, we‚Äôll search this database to find the most semantically similar document chunks and use them as context for generating answers.
</p>

  <pre class="bg-light p-3 rounded"><code>
from langchain.vectorstores import Chroma

vectorstore = Chroma.from_documents(chunks, embedding=embedding_model, persist_directory="./chroma_db")</code></pre>

  <h5 class="mt-4"> Initialize Ollama with Gemma 3</h5>
  <pre class="bg-light p-3 rounded"><code>
from langchain.llms import Ollama

llm = Ollama(model="gemma3")</code></pre>

<h4 class="mt-5">Graph-Based RAG with LangGraph</h4>
<p>This section walks through each part of the LangGraph-powered RAG implementation, explaining every node and design decision in detail.</p>

<h5 class="mt-4">üìå Imports and Definitions</h5>
<pre class="bg-light p-3 rounded"><code>from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document</code></pre>
<ul>
  <li><code>START</code>: Defines where the graph begins.</li>
  <li><code>StateGraph</code>: Constructs the application flow using nodes.</li>
  <li><code>TypedDict</code>: Allows structured input/output state sharing.</li>
  <li><code>PromptTemplate</code>: Formats context for the LLM.</li>
  <li><code>Document</code>: Represents each text chunk retrieved from your vector DB.</li>
</ul>

<h5 class="mt-4"> State Definition</h5>
<p>
The shared memory between graph nodes. Each node receives and returns this dictionary-like state.
</p>
<pre class="bg-light p-3 rounded"><code>class State(TypedDict):
    question: str
    context: List[Document]
    answer: str</code></pre>


<h5 class="mt-4">üßæ Prompt Template</h5>
<pre class="bg-light p-3 rounded"><code>
prompt = PromptTemplate.from_template("""
You are an expert assistant. Use the following context to answer the question.
If the answer cannot be found in the context, just say you don't know ‚Äî don't make anything up.

Context:
{context}

Question: {question}
Answer:
""")</code></pre>
<p>
This prompt keeps responses grounded and avoids hallucination by instructing the LLM to admit when it doesn't know the answer.
</p>

<h5 class="mt-4"> Node 1: <code>retrieve</code></h5>
<p>
This node performs semantic search using the input question. The top-matching documents are inserted into the <code>context</code>.
</p>
<pre class="bg-light p-3 rounded"><code>
  def retrieve(state: State):
    retrieved_docs = vectorstore.similarity_search(state["question"])
    return {"context": retrieved_docs}</code></pre>


<h5 class="mt-4">üí¨ Node 2: <code>generate</code></h5>
<p>
This node merges the retrieved docs, formats them using the prompt, and sends it to the LLM to get a final answer.
</p>
<pre class="bg-light p-3 rounded"><code>
  def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response}</code></pre>


<h5 class="mt-4"> Building the Graph</h5>
<p>
This configures your RAG application as a graph, starting from <code>retrieve</code> and flowing into <code>generate</code>. The graph is then compiled for execution.
</p>
<pre class="bg-light p-3 rounded"><code>
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()</code></pre>


<h5 class="mt-4">‚ùì Running a Query</h5>
<pre class="bg-light p-3 rounded"><code>response = graph.invoke({"question": "What is my name?"})
print(response["answer"])</code></pre>
<p>
This triggers the entire graph: retrieval ‚ûù prompt formatting ‚ûù answer generation ‚Äî and prints the final result.
</p>

<h5 class="mt-4"> Visualizing the Graph</h5>
<pre class="bg-light p-3 rounded"><code>from IPython.display import Image, display
display(Image(graph.get_graph().draw_mermaid_png()))</code></pre>
<p>
This generates a Mermaid-style flowchart image, showing the structure of your application.
</p>
  <div class="mb-4">
    <img src="../img/Blog_11_1.png" alt="embeddings" class="img-fluid rounded shadow" style="max-width: 400px; width: 10%; height: 25%;" />
</div>

<h5 class="mt-4">üß© Node Summary</h5>
<div class="table-responsive">
  <table class="table table-bordered table-striped">
    <thead class="table-dark">
      <tr>
        <th>Node</th>
        <th>Role</th>
        <th>Input</th>
        <th>Output</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>retrieve</code></td>
        <td>Finds relevant documents</td>
        <td><code>question</code></td>
        <td><code>context</code></td>
      </tr>
      <tr>
        <td><code>generate</code></td>
        <td>Generates final answer using retrieved context</td>
        <td><code>question + context</code></td>
        <td><code>answer</code></td>
      </tr>
      <tr>
        <td><code>graph</code></td>
        <td>Sequences all steps and manages state</td>
        <td><code>State</code></td>
        <td><code>Updated State</code></td>
      </tr>
    </tbody>
  </table>
</div>

<p class="fw-bold">
Each node is modular and self-contained, making your RAG app highly testable, maintainable, and extensible. 
</p>
<h4 class="mt-5">üìú Here‚Äôs the Complete Code</h4>
<p>
  Now that we‚Äôve explored each component step-by-step ‚Äî from loading PDFs and chunking text, to embedding and retrieval ‚Äî
  let‚Äôs put it all together using <strong>LangGraph</strong> to orchestrate the flow.
</p>
<p>
  Below is the full working code for building a RAG pipeline that retrieves context from documents and generates answers using <code>Gemma 3</code> via <code>Ollama</code>. 
  This modular, graph-based setup makes it easy to scale, debug, and extend.
</p>

<pre class="bg-light p-3 rounded"><code>
# Load the documents
import os
from langchain.document_loaders import PyPDFLoader

# Step 1: Load the pdfs
pdf_folder = "data/pdfs"
all_docs = []

for filename in os.listdir(pdf_folder):
    if filename.endswith(".pdf"):
        loader = PyPDFLoader(os.path.join(pdf_folder, filename))
        docs = loader.load()
        all_docs.extend(docs)

print(f"Total characters: {len(docs[0].page_content)}")
print(docs[0].page_content[:500])

# Split the text
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(all_docs)

print(f"Split resume pdfs into {len(all_splits)} sub-documents.")

# Embeddings
from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Vectorstore
from langchain.vectorstores import Chroma

vectorstore = Chroma.from_documents(all_splits, embedding=embedding_model, persist_directory="./chroma_db")
retriever = vectorstore.as_retriever()

document_ids = vectorstore.add_documents(documents=all_splits)
print(document_ids[:3])

# LLM Initialization
from langchain.llms import Ollama

llm = Ollama(model="gemma3")

# LangGraph Setup
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict
from langchain_core.documents import Document
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    """
    You are an expert assistant. Use the following context to answer the question.
    If the answer cannot be found in the context, just say you don't know ‚Äî don't make anything up.

    Context:
    {context}

    Question: {question}
    Answer:
    """
)

class State(TypedDict):
    question: str
    context: List[Document]
    answer: str

# Nodes
def retrieve(state: State):
    retrieved_docs = vectorstore.similarity_search(state["question"])
    return {"context": retrieved_docs}

def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response}

# Build and run the graph
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()

response = graph.invoke({"question": "What is my name"})
print(response["answer"])

# Visualize the graph
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))

# Additional invocations
result = graph.invoke({"question": "What is My email?"})

print(f'Context: {result["context"]}\n\n')
print(f'Answer: {result["answer"]}')

# Streaming responses
for step in graph.stream(
    {"question": "Provide"}, stream_mode="updates"
):
    print(f"{step}\n\n----------------\n")

</code></pre>
  <p class="fw-bold">
    LangGraph gives your RAG pipeline memory, logic, and structure ‚Äî making it far easier to scale and experiment. More features like streaming and condition-based flows are just a few lines away. üîÑ
  </p>
</div>
</body>
</html>
